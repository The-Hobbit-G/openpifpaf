#!/bin/bash -l

#SBATCH --nodes=4
#SBATCH --ntasks-per-node=2
#SBATCH --cpus-per-task=20
#SBATCH --mem=64G
#SBATCH --partition=gpu
#SBATCH --qos=gpu
#SBATCH --gres=gpu:2
#SBATCH --time=72:00:00
#SBATCH --output=/scratch/izar/jiguo/train/cocodet/resnet50/%x.out

shopt -s extglob

export MASTER_ADDR=$(hostname)
export MASTER_PORT=7247
export TORCH_DISTRIBUTED_DEBUG=INFO


echo STARTING AT `date`


xpdir="/scratch/izar/jiguo/train/cocodet/resnet50/${SLURM_JOB_NAME}"
mkdir -p ${xpdir}
# mkdir -p ${xpdir}/code
# tar -czvf ${xpdir}/code/code.tar.gz /home/jiguo/openpifpaf
mkdir -p ${xpdir}/logs


cd ..

mkdir -p ${xpdir}/checkpoints

srun --gpu-bind=closest /bin/bash -c "time python3 -m openpifpaf.train --ddp \
  --output ${xpdir}/checkpoints/${SLURM_JOB_NAME}.pt \
  --dataset=cocodet \
  --cocodet-square-edge=513 \
  --cocodet-extended-scale \
  --cocodet-orientation-invariant=0.1 \
  --cocodet-upsample=2 \
  --basenet=resnet50 \
  --cifdet-side-length=16 \
  --epochs=150 \
  --batch-size=8 \
  --lr=0.0001 \
  --lr-decay 130 140 \
  --lr-decay-epochs=10 \
  --momentum=0.9 \
  --weight-decay=1e-5 \
  --b-scale=10.0 \
  --clip-grad-value=10.0 \
  2>&1 | tee ${xpdir}/logs/${SLURM_JOB_NAME}.log"

cd -


echo FINISHED at `date`
